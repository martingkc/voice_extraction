{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12420728,"sourceType":"datasetVersion","datasetId":7833998}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Install Dependencies ","metadata":{}},{"cell_type":"code","source":"!pip install dmel torchaudio\n!git lfs install\n!git clone https://huggingface.co/nvidia/bigvgan_24khz_100band\n!pip install bigvgan resampy speechbrain\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Imports \n[](http://)","metadata":{}},{"cell_type":"code","source":"import os, re\nimport torch, torchaudio\nimport numpy as np\nimport librosa\nfrom IPython.display import Audio\nimport torch.nn.functional as F\nimport bigvgan\nfrom transformers import GPT2TokenizerFast\nfrom dmel import LogMelFbank, DiscretizedLogMelFbank\nimport scipy.signal as sig\nfrom speechbrain.inference.vocoders import HIFIGAN","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cpu\" if not torch.cuda.is_available() else torch.cuda.current_device()\n\nn_mels, n_bins = 80, 2**8\nTOKEN_REGEX = re.compile(r\"<c(\\d+)b(\\d+)>\")\n\nlogmelfbank = LogMelFbank(\n    sampling_freq=16000,\n    n_fft=2048,            \n    frame_size_ms=25,      \n    frame_stride_ms=12.5,    \n    n_filterbank=80,\n    window_type=\"HAMMING\",\n    device=device,\n    low_freq=0, \n    high_freq=9000,\n)\ndlogmelfbank = DiscretizedLogMelFbank(\n    logmelfbank=logmelfbank,\n    n_bits=8,  \n    quantize_min_value=-7.0,\n    quantize_max_value=2.0,\n    device=device,\n)\n\n\ndmel_tokens = [f\"<c{ch}b{b}>\" for ch in range(n_mels) for b in range(n_bins)]\n\n\nhifi_gan = HIFIGAN.from_hparams(\n    source=\"speechbrain/tts-hifigan-libritts-16kHz\",\n    savedir=\"pretrained_models/tts-hifigan-libritts-16kHz\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def wav_to_dmel_tokens(wav_path: str) -> list[str]:\n    wav, sr = torchaudio.load(wav_path)\n    if sr != 16000:\n        wav = torchaudio.transforms.Resample(sr, 16000)(wav)\n    pre_emph = 0.0\n    wav = wav.clone()\n    wav[:, 1:] = wav[:, 1:] - pre_emph * wav[:, :-1]\n    lengths = torch.LongTensor([wav.shape[-1]]).to(device)\n    all_toks, _ = dlogmelfbank(wav.to(device), lengths)\n    real = all_toks[0, 1:-1, :]\n    toks = []\n    for t in range(real.shape[0]):\n        for ch in range(n_mels):\n            b = int(real[t, ch])\n            toks.append(f\"<c{ch}b{b}>\")\n    return toks\n\ndef reconstruct_log_mel_with_apple(\n    token_strings: list[str]\n) -> torch.FloatTensor:\n    \"\"\"\n    \"<c{ch}b{bin}>\" tokens → (n_mels, frames) log-mel spectrogram (dB).\n    \"\"\"\n    # Extract only the bin indices (0…15)\n    bins = [int(_TOKEN_RE.fullmatch(tok).group(2)) for tok in token_strings]\n    \n    # Reshape to (1, T, n_mels)\n    frames = len(bins) // n_mels\n    bin_tensor = torch.LongTensor(bins).view(1, frames, n_mels)\n    \n    # Apple’s inverse discretizer → (1, T, n_mels) in dB\n    inv = dlogmelfbank.inv_discretize_func(bin_tensor)\n    \n    # Permute to (n_mels, frames)\n    return inv[0].T \n\ndef resample_spectrogram(spec: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Resamples a spectrogram along the time axis.\n    \n    Args:\n        spec (torch.Tensor): Spectrogram of shape (n_mels, frames).\n    \n    Returns:\n        torch.Tensor: Resampled spectrogram.\n    \"\"\"\n    # F.interpolate expects input of shape (N, C, W)\n    # Our spec is (n_mels, frames), which is (C, W). We add a batch dim N=1.\n    spec_for_interp = spec.unsqueeze(0)\n    \n    # Resample along the last dimension (width/time)\n    resampled_spec = F.interpolate(\n        spec_for_interp,\n        scale_factor=200/256, # (1, 0.78125)\n        mode='linear', # Bilinear is good for 2D data (mels x time)\n        align_corners=False,\n    )\n    \n    # Remove the batch dimension before returning\n    return resampled_spec.squeeze(0)\n\n\ndef tokens_to_waveform(tokens: list[str]) -> torch.Tensor:\n    spec = reconstruct_log_mel_with_apple(tokens)\n    resized = resample_spectrogram(spec).unsqueeze(0)\n    \n    \n    \n    with torch.inference_mode():\n        wav_gen = hifi_gan.decode_batch(resized)\n\n         \n    return wav_gen\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wav_path = \"/kaggle/input/example/example.wav\"\ntokens   = wav_to_dmel_tokens(wav_path)[n_mels:-n_mels]\nwav = tokens_to_waveform(tokens).squeeze(1)\nwaveform = wav.clone().to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import scipy.signal as sig\n\npitch_shift = torchaudio.transforms.PitchShift(\n    sample_rate=16000,\n    n_steps=1,           # positive = higher pitch\n    n_fft=2048,          # FFT window size for phase vocoder\n    bins_per_octave=30            # 12 bins per octave\n).to(device)\nwaveform[:, 1:] = waveform[:, 1:] + 0.0 * waveform[:, :-1]\n\nwav_high = pitch_shift(waveform)\nprint(len(wav[0]))\ny_wien = sig.wiener(wav_high.detach().cpu(), mysize=29, noise=0.01)\ndisplay(Audio(y_wien[0], rate=16000))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}